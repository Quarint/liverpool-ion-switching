# Liverpool-ion-switching-aws

This repository contains my tries at predicting the number of ion open channels from electrical data in cells, for the Kaggle competition by University of Liverpool - Ion Switching.

Rather than trying at all costs to get the highest possible score on this competition, I used it to practice a few ML algorithms. Indeed, as can be seen on the [Kaggle leaderboard](https://www.kaggle.com/c/liverpool-ion-switching/leaderboard), the competition is either too simple and has been solved by pretty much everyone already, or it is too difficult and absolutely everyone is stuck around the 0.940 threshold.

My objectives here are therefore the following: 
* Practice some data cleaning and manipulation *in an automated way*. A good work was done by a few Kagglers on the original data for this competition, which contained data drift. Indeed, both the training and the test data are made of several chunks of data, to which specific noise and drift were applied, and then the chunks were concatenated together. A few people already did the work of removing the drift from both the training and the test data, and they did it well. Therefore, pretty much everyone on Kaggle is working from this public cleansed data instead of the original data. As for myself, I tried to do my own cleansing of the data, which can be found in the Exploratory Analysis notebook. My cleansing process mostly uses low-pass Butterworth filters which removes the highest frequencies in the data, which correspond to the drift that was artificially added. I also tried remove the noise with high-pass filters, but this has a negative impact on the forecast and was therefore not used in the end. A special point of attention for me was to automatize the whole ML pipeline, from data cleaning to feature engineering and prediction, in order to simulate industrial conditions. Indeed, the cleansing process that was applied to both the cleansed training data and the cleansed train data that Kagglers use, was not automated. Human intervention was applied to split both datasets into chunks, and remove the drift from each chunk manually. Even in the feature engineering part of the pipeline, many notebooks manually chose to apply different ML algorithms to different chunks of the test data. Although this might be an easier way to get a higher Kaggle score, it does provide good practice for applying ML in real life conditions. So even if my own cleansed data is just a bit less perfect than the canon cleansed data, I'm pretty satisfied with it because it was cleansed in an automated way.
* Get practice with feature engineering (still in an automated way). This mainly takes place in the KNN Classifier and the Random Forest notebooks. The successful features that I've added are of two types :
    * Shifted versions of the lone original variable, in both directions. Experiments revealed that all shifted versions are useful up to a shift of 10 time periods in both directions. 
    * The number of the chunk of 50k samples to which the current sample belongs. Indeed, since I don't want to train different classifiers for each chunk of training data and then manually chose a classifier to apply to each chunk of the test data, a solution is to give the (only) classifier a feature that tells him in which chunk he is currently working. Then, for each chunk of the test data, the number of the closest chunk (with the euclidian distance) of the training data is computed and this value is used as the corresponding feature for prediction on the test data. This inclusion was very successful, as I reached a score of 0.938 with the first random forest model I trained with it, pretty to 0.946 which is currently the best score on the Kaggle leaderboards. I hope to get a bit closer with XGBoost and / or ensembling.
* Get some practice by trying a few different ML models. Instead of XGBoosting my way to a high score, I tried simple models such as KNN and decision trees first. I'm currently trying more elaborate, ensemble models such as random forest and xgboosting, and maybe I'll try other models afterwards.
* Get more familiar with AWS S3 and Sagemaker. Although I've already went though the Sagemakers tutorials on AWS documentation, I wanted to a real project this time, storing data on S3 and then retrieving it on Sagemaker to train a model and expose an endpoint for prediction. AWS takes a little time to get used to but I think I'm getting a better grasp on it now.
